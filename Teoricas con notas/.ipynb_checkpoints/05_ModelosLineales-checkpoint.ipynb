{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Lineales - Parte 1. Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de ajuste lineal\n",
    "\n",
    "Para empezar a ganar confianza con los modelos lineales, vamos a ajustar dos modelos: uno lineal y otro polinomial. \n",
    "Para cancherear, vamos a decir que vamos a entrenar los modelos con datos sintéticos, y vamos a evaluar la performance del modelo con otro set de datos (datos de test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como de costumbre, primero hacemos algunas importanciones y tuneos (esto está sacado casi literal del Géron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"05_ModelosLineales\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos los datos sintéticos, a partir de un modelo conocido (ground truth), con coeficiente $m$ y $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rr\n",
    "\n",
    "# Parámetros de la ground truth\n",
    "b = 4\n",
    "m = 5\n",
    "\n",
    "# Número de datos\n",
    "n = 100\n",
    "\n",
    "# \n",
    "x_ = 2 * np.random.rand(n, 1)\n",
    "\n",
    "# El modelo real (ground truth)\n",
    "t_ = b + m * x_\n",
    "\n",
    "# Agregemos error normal a los datos\n",
    "t_ += np.random.randn(n, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, vamos a separar 20% de los datos para usar de test, y quedarnos solo con el 80% de datos de entrenamiento. En el contexto de un ajuste lineal, esto parace una exageración de nomenclatura, pero es la que vamos a usar después para casos más complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_test(data, fraccion, seed=1234):\n",
    "\n",
    "    # Fijar el seed para ser reproducible\n",
    "    import numpy.random as rr\n",
    "    if seed is not None:\n",
    "        rr.seed(seed)\n",
    "    \n",
    "    # Permuta los índices para elegir al azar\n",
    "    ind = rr.permutation(len(data))\n",
    "    # Calcula el número de elementos en el set de test\n",
    "    ntest = int(len(data) * fraccion)\n",
    "    \n",
    "    # Construye los índices para cada conjunto\n",
    "    indices_train = ind[ntest:]\n",
    "    indices_test = ind[:ntest]\n",
    "    return data[indices_train], data[indices_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una implementación muy similar aparece en el paquete <tt>Scikit-Learn</tt>, que vamos a usar mucho de ahora en más. Veamos que ambas cosas dan lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construye array con los x y los t\n",
    "data = np.hstack([x_, t_])\n",
    "\n",
    "# Nuestra implementación\n",
    "data_train, data_test = elegir_test(data, 0.2)\n",
    "\n",
    "# Implementación en scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "sk_train, sk_test = train_test_split(data, test_size=0.2, random_state=1234)\n",
    "\n",
    "for tipo, nos, sk in zip(['Train', 'Test'], [data_train, data_test],[sk_train, sk_test]):\n",
    "    print('Los conjuntos de {} son idénticos: {}.'.format(tipo, np.allclose(nos, sk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y veamos finalmente a qué se parecen los datos de entrenamiento\n",
    "plt.plot(data_train[:, 0], data_train[:, 1], '.')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ajustar los datos con un modelo lineal simple (que además es el modelo correcto):\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, w_0, w_1) = w_0 + w_1 x\\;\\;.\n",
    "$$\n",
    "\n",
    "En la nomenclatura que vimos en la clase, tenemos $\\phi_0(\\mathbf{x}) = 1$, y $\\phi_1(\\mathbf{x}) = x$, y en ese caso recuperamos la ecuación conocida:\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\mathbf{w}) = \\sum_{j=0}^1 w_j \\phi_j(\\mathbf{x})\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suposición del modelo es que la variable $t$ puede escribirse como \n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x}, \\mathbf{w}) + \\epsilon\\;\\;,\n",
    "$$\n",
    "donde suponemos que el error $\\epsilon$ es una variable aleatoria, distribuida como una Gaussiana centrada en cero y con precisión $\\beta$.\n",
    "\n",
    "En ese caso, la función de distribución para $t$ es:\n",
    "\n",
    "$$\n",
    "p(t \\vert \\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N}(t \\vert y(\\mathbf{x}, \\mathbf{w}), \\beta^{-1})\\;\\;,\n",
    "$$\n",
    "por lo que, usando las propiedades de la Gaussiana, vemos que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[t|x] = y(\\mathbf{x}, \\mathbf{w})\\;\\;,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos que la solución que maximiza la verosimilitud se obtiene a través de las ecuaciones normales:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_\\mathrm{ML} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{t}\\;\\;,\n",
    "$$\n",
    "donde $\\mathbf{\\Phi}$ es la llamada matriz de diseño, que para este caso es muy sencilla:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi} = \n",
    "\\begin{pmatrix} \n",
    "1 & x[0] \\\\\n",
    "1 & x[1] \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x[n-1] \\\\\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "\\;\\;.\n",
    "$$\n",
    "\n",
    "Calculemos la solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_train[:, 0].reshape(len(data_train), 1)\n",
    "t = data_train[:, 1].reshape(len(data_train), 1)\n",
    "\n",
    "# Primero definamos la matriz uniendo dos vectores\n",
    "phi = np.hstack([x*0.0 + 1, x])\n",
    "\n",
    "# Ahora calculemos el producto de phi por su transpuesta y verifiquemos que la forma es la correcta\n",
    "pp = np.dot(phi.T, phi)\n",
    "\n",
    "# y el producto entre phi y el vector t\n",
    "yy = np.dot(phi.T, t)\n",
    "\n",
    "print(phi.shape, pp.shape, yy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, en lugar de calcular la inversa y multiplicar, resolvemos el sistema:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right) \\mathbf{w}_\\mathrm{ML} = \\mathbf{\\Phi}^T \\mathbf{t}\\;\\;.\n",
    "$$\n",
    "\n",
    "Para eso, usamos <tt>np.linalg.solve</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml = np.linalg.solve(pp, yy)\n",
    "print(wml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que obtenemos valores parecidos al modelo verdadero. Podemos calcular alguna medición del alejamiento de los datos de entrenamiento del modelo. Lo típico, justificado por la maximización de la verosimilitud, es calcular el Root-Mean-Square Error (RMSE):\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(y(\\mathbf{x_i}, \\mathbf{w_\\mathrm{ML}}) - t_i\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(t, tpred):\n",
    "    return np.sqrt(np.sum((tpred - t)**2) / len(tpred))\n",
    "\n",
    "print('El RMSE en el conjunto de training es: {:.3f}'.format(rmse(t, phi @ wml)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, t, '.')\n",
    "\n",
    "# Calculemos la predicción del modelo sobre los datos de testeo\n",
    "x_test = data_test[:, 0].reshape(len(data_test), 1)\n",
    "phi_test = np.hstack([x_test*0.0 + 1, x_test])\n",
    "\n",
    "# La predicción\n",
    "t_pred = phi_test @ wml\n",
    "\n",
    "# Vean que lo que estamos haciendo es multiplicar cada elemento de wml por \n",
    "# la columna correspondiente de la matriz de diseño, y sumando\n",
    "t_pred2 = np.array([wml[i] * phi_test[:, i] for i in range(phi.shape[1])])\n",
    "t_pred2 = np.sum(t_pred2, axis=0)\n",
    "\n",
    "# Veamos que es lo mismo\n",
    "print('Hacer ambas cosas es lo mismo: {}'.format(np.allclose(t_pred, t_pred2[:, np.newaxis])))\n",
    "\n",
    "plt.plot(x_test, t_pred, '-r')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, podemos calcular el RMSE del conjunto de testeo, usando el valor verdadero que habíamos generado, <tt>t_test</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = data_test[:, 1].reshape(len(data_test), 1)\n",
    "\n",
    "print('El RMSE en el conjunto de training es: {:.3f}'.format(rmse(phi_test @ wml, t_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Ejemplo de ajuste multivariado\n",
    "\n",
    "Vamos a hacer un ejemplo práctico que utiliza modelos lineales para predecir el valor de casas en California. Es un ejemplo clásico, que está descripto con detalle en el libro de Géron.\n",
    "\n",
    "### Objetivo\n",
    "El objetivo es entrenar un algoritmo que permita predecir el precio de las casas en California, usando datos de un censo. El censo provee información sobre el costo medio de las casas en cada distrito cubierto por el censo, junto con otros datos relevantes, como cercanía al mar, e ingreso promedio.\n",
    "\n",
    "El sistema tiene el objetivo de remplazar estimaciones hechas a mano por expertos, que en general salen muy mal.\n",
    "\n",
    "En este punto, Géron nos invita a pensar sobre el tipo de algoritmo que tenemos que implementar:\n",
    "\n",
    "- ¿Es supervisado o no supervisado? ¿Alguna otra cosa?\n",
    "\n",
    "- ¿Debe ser un algoritmo de clasificación o de regresión? ¿O Alguna otra cosa?\n",
    "\n",
    "- ¿Podemos aprender en batch, o sería mejor algo de aprendizaje online (es decir, progresivo)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de los datos\n",
    "\n",
    "Vamos a bajar los datos del repositorio (sólo si estás corriendo esto en Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/exord/UNSAM_IA/master/\"\n",
    "HOUSING_PATH = os.path.join(\".\", \"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    #urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    !wget http://raw.githubusercontent.com/exord/UNSAM_IA/master/datasets/housing.tgz -P {housing_path}\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es la costumbre, después de cargar los datos vamos a ver las primeras filas, y usemos el método <tt>info</tt> para tener más información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la tabla tiene 20640 entradas, con 10 columnas cada una. Todas las columnas son numéricas menos la última \"ocean_proximity\". Además, vemos que hay algunos registros faltantes en  \"total_bedrooms\". \n",
    "\n",
    "En el head no se puede ver los valores que toma la columna \"ocean_proximity\"; veamos qué valores hay y cuántos de cada uno. Para eso, usamos el método <tt>value_counts</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos un poquito más allá y exploremos los datos numéricos un poco. Para eso, existen métodos de <tt>pandas</tt> que son muy útiles. Algunos ya los vimos, como <tt>describe</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas cosas:\n",
    "\n",
    "- El método evita los valores que no están presentes (por eso el \"count\" de \"total_bedrooms\" es 20433).\n",
    "\n",
    "- Como verán, el ingreso medio fue preprocesados, para tener valores entre 0.5 y 15.0, en lugar de los verdaderos ingresos.\n",
    "\n",
    "Otro método que está muy bueno es uno que te permite en un comando hacer los histogramas de todos los valores numéricos. Se trata de <tt>hist</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20,15))\n",
    "save_fig(\"attribute_histogram_plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas cosas más:\n",
    "\n",
    "- La edad media de las casas y el precio tienen un valor máximo. Esto se ve porque el último bin del histograma tiene muchos elementos. Esto es un problema para predecir valores por encima de $500'000.\n",
    "\n",
    "- Algunos histogramas tienen colas muy largas, que van a ser problemáticas a la hora de hacer predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del conjunto de entrenamiento y de testeo\n",
    "\n",
    "Ahora vamos a separar en dos la tabla de datos, definiendo un conjunto de entrenamiento y uno de testeo. Para eso, usamos la función de <tt>Scikit-Learn</tt> que vimos un poco más arriba (fíjense que funciona tanto para array como para dataframes de <tt>pandas</tt>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=445543)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijar el <tt>seed</tt> ayuda a que si corremos el algoritmo nuevamente, siempre llegemos a los mismos conjuntos. De lo contrario, cabe el riesgo que por correr muchas veces el algoritmo, el conjunto de prueba quede revelado.\n",
    "\n",
    "Hay mucho más para decir de cómo se elige el conjunto de prueba, pero lo vamos a ir introduciendo de a poco en varios ejemplos a lo largo del curso.\n",
    "\n",
    "Una cosa que vamos a discutir ahora rápidamente es la de elegir de forma estratificada en las dimensiones que sean relevantes. Supongamos que por alguna razón, creemos que una de las variables que serán determinantes en el precio de las casa sea el ingreso medio. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora dividámoslo en categorias. Por ejemplo, cada 1.5 unidades de ingreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, de nuevo, podemos usar la magia de Scikit Learn para elegir conjuntos que respeten la proporción. Para eso, usamos la clase <tt>StratifiedShuffleSplit</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=445543)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos ahora la fracción de casos en cada una de las cetegorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la cosa funciona: la fracción en cada categoría se mantiene.\n",
    "\n",
    "Géron nos da una mano para mostrarnos la diferencia con haber elegido al azar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=1234)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Desafío (interludio)\n",
    "\n",
    "Sin avanzar más en la exploración de datos, elijan un feature de la lista e intenten repetir lo que hicimos arriba para ajustar los valores de las casas con ese feature. \n",
    "\n",
    "Comparen el resultado usando el RSME\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de los datos\n",
    "\n",
    "Antes de lanzarnos a entrenar un algoritmo de inteligencia artificial, es conveniente mirar los datos un poco más de cerca (el conjunto de testeo) para ganar alguna idea de cuál es el contenido.\n",
    "\n",
    "Pero primero copiemos el set de entrenamiento, así podemos jugar sin miedo a romper nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "#save_fig(\"better_visualization_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede agregar mucha más información al scatterplot usando los tamaños ($s$) y los colores ($c$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular el coeficiente de correlación de Pearson ($\\rho$) para cada par de columnas:\n",
    "\n",
    "$$\n",
    "\\rho_{ij} = \\frac{\\mathrm{cov}(X_i, X_j)}{\\sqrt{\\mathrm{var}(X_i) \\mathrm{var}(X_j)}}\n",
    "$$\n",
    "\n",
    "Si bien este coeficiente solo indica correlaciones lineales, nos da una idea de las dependencias de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver los plots de los datos de a pares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "#save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concentrarnos en la correlación más fuerte, entre ingreso medio y valor medio de la casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "#save_fig(\"income_vs_house_value_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos varias cosas que ya habíamos mencionado:\n",
    "\n",
    "- Una buena correlación.\n",
    "\n",
    "- El hecho de que el valor máximo catalogado es de medio millón de dólares (vaya...)\n",
    "\n",
    "- También vemos que hay algunas líneas horizontales, en distintos valores.\n",
    "\n",
    "Todo esto lo vamos a tener que arreglar para poder trabajar con estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, podemos pensar en combinar las variables para tener elementos que sean más significativos.\n",
    "\n",
    "#### Pregunta\n",
    "\n",
    "¿Además de los que aparecen acá abajo, se les ocurre alguno más?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de cuartos por casa\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "# Cantidad de cuartos por habitación\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "# Cantidad de población por casa\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si alguno de estos tiene buena correlación con el precio medio de las casas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una serie de tareas, la mayoría muy aburridas, que habría que hacer en el conjunto de datos que tenemos:\n",
    "\n",
    "- Completar o filtrar datos faltantes (por ejemplo en el número de habitaciones totales, que vimos más arriba).\n",
    "\n",
    "- Codificar los datos no numéricos (como la \"proximidad a la playa\").\n",
    "\n",
    "- Uniformizar la escala de las \"features\".\n",
    "\n",
    "Todo esto requiere una buena dosis de codeo. Por suerte, <tt>Scikit-Learn</tt> tiene toda un conjunto de Transformers que ayudan a hacer esto.\n",
    "\n",
    "Por lo pronto, vamos a tomar el camino fácil, y nos vamos a deshacer de las filas que tienen datos faltantes y de las columnas con texto. \n",
    "\n",
    "(Ya veremos, con detalle, el uso de las clases de Scikit Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos la condición de que ninguna columna esté vacía\n",
    "# Para eso, buscamos el contrario de que cualquiera esté vacía.\n",
    "# Por supuesto, lo mismo tenemos que hacer para las labels.\n",
    "cond = ~housing.isnull().any(axis=1)\n",
    "housing_tr = housing[cond]\n",
    "housing_labels = housing_labels[cond]\n",
    "\n",
    "# Ahora tiramos a la basura el feature \"sea_proximity\"\n",
    "housing_num = housing_tr.drop('ocean_proximity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estandarizamos los datos. Es decir, a cada columna le restamos su media y dividimos por su desviación estándard. Esto es típico y ayuda a la convergencia de los algoritmos.\n",
    "\n",
    "Usamos más magia de <tt>ScikitLearn</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "hh = scaler.fit_transform(housing_num)\n",
    "\n",
    "# Esto devuelve un array, que hay que transformar de nuevo a un DataFrame\n",
    "housing_prepared = pd.DataFrame(hh, columns=housing_num.columns)\n",
    "housing_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de un modelo lineal\n",
    "\n",
    "Ahora tenemos los datos listos para usar en un modelo de regresión lineal.\n",
    "Vamos a usar la implementación de SciKit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las predicciones de algunas filas y los valores reales\n",
    "some_data = housing_prepared.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "\n",
    "print(\"Predicciones:\", lin_reg.predict(some_data))\n",
    "print(\"Labels:\", list(some_labels))\n",
    "\n",
    "print(\"Diferencias porcentuales:\", list(100*np.abs(lin_reg.predict(some_data) - some_labels)/some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a medir el valor del RMSE usando todo el set de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que el error medio en la predicción de los valores es de casi $70000. No es muy bueno, pero es lo que hay con un modelo tan simple.\n",
    "\n",
    "Es interesante ver la distribución de las diferencias absolutas entre las predicciones y los labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abserror = np.abs(housing_predictions - housing_labels)\n",
    "print('Max', abserror.max())\n",
    "print('Median', np.median(abserror))\n",
    "print('25th-percentile', np.percentile(abserror, 25))\n",
    "print('75th-percentile', np.percentile(abserror, 75))\n",
    "print('90th-percentile', np.percentile(abserror, 90))\n",
    "\n",
    "h = plt.hist(np.abs(housing_predictions - housing_labels), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
    "lin_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación en el conjunto de test\n",
    "\n",
    "Ahora retomemos el conjunto de test y evaluemos las predicciones. \n",
    "Por supuesto, antes tenemos que hacer el mismo procesamiento a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test = strat_test_set.drop(\"median_house_value\", axis=1) # drop labels for test set\n",
    "housing_test_labels = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "cond = ~housing_test.isnull().any(axis=1)\n",
    "housing_test_tr = housing_test[cond]\n",
    "housing_test_labels = housing_test_labels[cond]\n",
    "\n",
    "# Ahora tiramos a la basura el feature \"sea_proximity\"\n",
    "housing_test_num = housing_test_tr.drop('ocean_proximity', axis=1)\n",
    "\n",
    "# Usamos el mismo scaler (ahora el método es fit en lugar de fit_transform)\n",
    "hh = scaler.transform(housing_test_num)\n",
    "\n",
    "# Esto devuelve un array, que hay que transformar de nuevo a un DataFrame\n",
    "housing_test_prepared = pd.DataFrame(hh, columns=housing_test_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, finalmente, calculemos las predicciones con nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test_predictions = lin_reg.predict(housing_test_prepared)\n",
    "\n",
    "abserror = np.abs(housing_test_predictions - housing_test_labels)\n",
    "\n",
    "print('Test Absolute Differences')\n",
    "print('Max', abserror.max())\n",
    "print('Median', np.median(abserror))\n",
    "print('25th-percentile', np.percentile(abserror, 25))\n",
    "print('75th-percentile', np.percentile(abserror, 75))\n",
    "print('90th-percentile', np.percentile(abserror, 90), '\\n')\n",
    "\n",
    "lin_mse = mean_squared_error(housing_test_labels, housing_test_predictions)\n",
    "print('RMSE (test)', np.sqrt(lin_mse))\n",
    "print('MAE (test)', mean_absolute_error(housing_test_labels, housing_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
