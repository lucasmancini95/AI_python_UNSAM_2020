{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYWroXTLs-Zp"
   },
   "outputs": [],
   "source": [
    "# Por si alguien corre en python2\n",
    "from __future__ import division\n",
    "\n",
    "# Preparamos todo para correr\n",
    "import numpy as np\n",
    "from math import *\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm, binom, gamma, poisson, multivariate_normal\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFmouu1ctIFf"
   },
   "source": [
    "## Ejercicio 1:Fitteando una recta ruidosa de manera Bayesiana.\n",
    "\n",
    "Como primer ejemplo de regresión lineal, tratemos de fittear los parametros $a_{0}$, $a_{1}$ de una recta $f(x,\\vec{a}) = a_0 + a_{1}x$ donde $x$ está en el intervalo $[-1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmazAIfPtWne"
   },
   "source": [
    "* **A)** Genere un set artificial de datos usando, usando como valores $a_{1} = 0.5$, $a_{0}= -0.3$. Añada a las mediciones ruido gaussiano  $\\epsilon \\sim \\mathcal{N}(\\mu=0, \\sigma = 0.2)$. \n",
    "Al terminar debería tener un conjunto de $N= 100$  pares $(x_n,t_n)$, con \n",
    "$t_n = f(x_n,\\vec{a}) + \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky5iN8XrtIZW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWYDby8puqkv"
   },
   "source": [
    "* **B)** Como modelo usaremos una regresión lineal de la forma $y(x) = \\omega_0 + \\omega_1 x$. ¿Cuál base de funciones $\\phi_j$ estamos usando? (recuerde que por convención $\\phi_0(x)=1$). \n",
    "Escriba para estos la matriz de diseño.\n",
    "$$\\Phi = \\begin{pmatrix}\n",
    "\\phi_0(x_1) & \\phi_1(x_1)\\\\\n",
    "\\phi_0(x_2) & \\phi_1(x_2)\\\\\n",
    "\\vdots & \\vdots)\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N)\\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xhcx-wbnusXa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QF8iFGk2utID"
   },
   "source": [
    "* **C)** Estamos interesados en encontrar los valores de $\\omega_0$ y $\\omega_1$ de nuestro modelo más probables, dado los datos que tenemos. Estos están dados por el máximo de nuestra distribución posterior. Si usamos priors uniformes en $\\omega_0$ y $\\omega_1$ para caracterizar nuestro desconocimiento, el máximo del posterior coincide con el de la verosimilitud y es lo que llamamos el *estimador de máxima verosimilitud*. De acuerdo a lo visto en la teórica, dicho valor esta dado por \n",
    "$$\\begin{pmatrix}\n",
    "\\omega_0^\\text{ML}\\\\\n",
    "\\omega_1^\\text{ML}\\\\\n",
    "\\end{pmatrix} \n",
    "= \\left(\\Phi^T \\Phi\\right)^{-1}\\Phi^T \\,\\vec{t}\n",
    "$$\n",
    "donde recordemos que $\\vec{t} = \\begin{pmatrix}t_1\\\\ \\vdots \\\\ t_N\\end{pmatrix}$ es el vector de los valores *target* medidos.\n",
    "\n",
    "Nota: Para un modelo lineal con error gaussiano, este estimador de maxima verosimilitud coincide con lo que se conoce como la solución de *cuadrados mínimos*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OWcChEAuuU9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0wuicKJxbPv"
   },
   "source": [
    "* **C)** Si en vez de utilizar un prior uniforme, utilizamos un prior gaussiano de la forma $p(\\vec{\\omega}) = \\mathcal{N} (\\vec{0},\\alpha^{-1}{\\bf 1})$ (es decir que $p(\\vec{\\omega}) = p(\\omega_0) \\times p(\\omega_1)$ con cada $p(\\omega_i) = \\mathcal{N} (\\mu=0,\\sigma = \\alpha)$ ). Calcule la verosimilitud y el posterior (prior*posterior normalizado) al usar solo 1 punto, 2 puntos, 3 puntos y todo el conjunto de 100 puntos.\n",
    "\n",
    "\n",
    "Si se le complica hacerlo en forma numerica, puede utilizar la formula analitica dle posterior: Como este es un prior conjugado a la gaussiana en la teórica (o en su defecto, en el Bishop ecuacion 3.53), vimos que nuestro posterior es una gaussiana con valor medio\n",
    "$$\n",
    "\\vec{m}_N = \\beta {\\bf S}_N \\Phi^T \\vec{t}\\\\\n",
    "{\\bf S}_N^{-1} = \\alpha {\\bf 1} + \\beta \\Phi^T \\Phi\n",
    "$$\n",
    "donde $\\beta$ es el parámetro de precisión del ruido gaussiano, que se supone conocido. En nuestro caso sería $\\beta = (1/\\sigma)^2 = (1/0.2)^2 = 25$. Para seleccionar la cantidad de puntos a considerar, puede usar slicing en $\\vec{t}$ ( ``t[:N_puntos]``) y en $\\Phi$ (``Phi[:N_puntos,:]``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMvwAd6K0VLX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PzLAyAa_HMO"
   },
   "source": [
    "# Ejercisio 1 bis: Scikit-learn\n",
    "\n",
    "Scikit-learn es una librería de machine learning con gran soporte para una multitud de algoritmos de análisis de datos, que nos permite acceder a ellos de una forma muy similar, obviando las diferencias técnicas entre sus implementaciones. En la práctica, nadie escribe sus propias funciones para algoritmos (a menos que sea un algoritmo novedoso que estás diseñando), sino que utilizamos las implementaciones provistas por este u otro paquete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XufupgU5_t6T"
   },
   "source": [
    "* **A)** Scikit-learn provee una clase LinearRegressor la cual permite obtener las soluciones de cuadrados mínimos en problemas lineales. Importe el paquete, y examine las distintas funciones leyendo la documentación provista en el paquete. Si no es suficiente, [visite la documentación on-line](https://scikit-learn.org/), para familiarizarse con como se utiliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dD8ficdARWP"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "#descomente cada linea para ver la documentación.\n",
    "#LinearRegression?\n",
    "#reg.fit?\n",
    "#reg.predict?\n",
    "#reg.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPnYV5bXAc2k"
   },
   "source": [
    "* **B)** Utilizando los datos del ejercisio anterior, repita el inciso **C)** utilizando scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xruB3zXLAcVx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQF-aGNL4PoM"
   },
   "source": [
    "# Ejercicio 2: Algoritmo de Cuadrados Mínimos para encontrar la mejor base de funciones\n",
    "\n",
    "El algoritmo de cuadrados mínimos consiste en hallar los parámetros que minimicen la distancia cuadrática entre los datos y mi ajuste $$ E_{D}(\\vec{w})=\\frac{1}{2}(t-\\vec{w}^{T}\\vec{\\phi}(\\vec{x}))^{2}.$$ Este puede verse como el estimador de máxima verosimilitud cuando modelamos los errores como gaussianos en un problema lineal.\n",
    "\n",
    " Considere el siguiente set de datos ``X``y ``T`` como dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1586200191591,
     "user": {
      "displayName": "Ignacio Fabre",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg7zSD7Nr148uG3EE-pBT8Q465capkzPkaSGwsEoM4=s64",
      "userId": "03651723443658892757"
     },
     "user_tz": 180
    },
    "id": "nTmm_AQT4Sj2",
    "outputId": "448a7ca5-e2be-47ee-c9c0-c50710ba87dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fedbc6f3f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=-1.0+2.0*np.random.rand(100)\n",
    "T=np.asarray(list(map(lambda x: 1.0*np.sin(6*x)+0.0*norm.rvs(loc=0.0,scale=0.1),X)))\n",
    "T.shape\n",
    "plt.scatter(X,T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51HYcOYa572S"
   },
   "source": [
    "* **A)** Considere la base de funciones los polinomios $\\phi_j(x) = x^j$, con $j=1,\\dots,M$. Calcule la matriz de diseño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADlzSf7Z5eei"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZH04_o1a7LQS"
   },
   "source": [
    "* **B)** Calcule la solución de cuadrados mínimos, utilizando la expresión dada en el inciso **C)** del **Ejercicio 1**, (la ecuación 3.15 del Bishop; ecuaciones normales). Grafíquela encima de los datos. Estudie como cambia para distintos tamaños de la base, i.e. distintos valores de $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22815mUD7dfj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Tib1NLO7eEJ"
   },
   "source": [
    "* **C)** Una forma de aproximar la solución cerrada de forma iterativa consiste en aplicar el algoritmo de Descenso por Gradiente para minimizar la función $E_D(\\vec{\\omega})$:\n",
    "$$\n",
    "\\vec{w}^{\\tau+1}=\\vec{w}^{\\tau}+\\eta\\nabla E_{n}(\\vec{w}^{\\tau})\\\\\n",
    "=\\vec{w}^{\\tau}+\\eta(t_{n}-\\vec{w}^{\\tau T}\\vec{\\phi}(\\vec{x}_n))\\vec{\\phi}(\\vec{x}_{n}).\n",
    "$$\n",
    "Donde partimos de un valor $\\vec{\\omega}^0$ aleatorio que nos permite calcular de forma iterativoa los $\\vec{\\omega}^\\tau$ subsiguientes en función del parametro $\\eta$ (también llamado *learning rate*). Examine como se va acercando este algoritmo a la solución cerrada calculada en el inciso anterior, para un valor de $\\eta=0.2$. Estudie como varia al cambiar $\\eta$ y de $\\omega_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpmXmr1p8chh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StnkcHgg9Umf"
   },
   "source": [
    "* **D)** Se puede ver que incluir un prior normal centrado en cero para los parámetros, equivale a agregar a la función de cuadrados mínimos un término regularizador\n",
    "$$E_{D}(\\vec{w})=\\frac{1}{2}\\sum_{i=1}^{N}(t_{i}-\\vec{w}^{T}\\vec{\\phi}(\\vec{x_{i}}))^{2}+\\frac{\\kappa}{2}\\vec{w}^{T}\\vec{w}.$$\n",
    "Esto se traduce, en el algoritmo recursivo, en la siguiente expresión\n",
    "$$\n",
    "\\vec{w}^{\\tau+1}=\\vec{w}^{\\tau}+\\eta\\left[(t_{n}-\\vec{w}^{\\tau T}\\vec{\\phi}(\\vec{x}_n))\\vec{\\phi}(\\vec{x}_{n}) + \\kappa \\vec{\\omega}^{\\tau}\\right].\n",
    "$$\n",
    "Modifique el código del inciso anterior, y para un valor de $\\kappa$ de su elección, estudie que sucede (puede comparar con el caso $\\kappa=0$ que coincide con el inciso anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNC_K7wI9k_p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeXfUP-f-cfu"
   },
   "source": [
    "* **E)** Separe el set de datos en training y validation en una proporción de 0.8/0.2. En base a lo visto en la teoría, ¿observa overfitting? ¿a que se debe? ¿cómo lo reduciría? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G27ycGcx-vC0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvRYHGmgA_5g"
   },
   "source": [
    "# Ejercicio 2 bis: Scikit-learn\n",
    "\n",
    "Utilizar una base de funciones $\\phi_j(x)$ es lo que en la jerga de data science se conoce como *feature extraction*. Transformamos nuestros datos $x$ en features $\\tilde x = \\phi_j(x)$, los cuales usamos para alimentar nuestros modelos (por ejemplo, el Regresor Lineal de los ejercicios anteriores. Este tipo de transformaciones es parte de lo que se conoce como *pre-procesado* de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULJIAyVjCRca"
   },
   "source": [
    "* **A)** Importe de ``sklearn.preprocessing`` la clase ``PolynomialFeatures``. Examine su documentación para ver que es lo que hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4j5wJkkBArO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osaITEz4CpPV"
   },
   "source": [
    "* **B)** Utilizando ``PolynomialFeatures`` y ``LinearRegressor`` repita el inciso **C** del ejercicio anterior. \n",
    "\n",
    "Si le interesa aprender una forma mas elegante de tratar con preprocesado (que podría ser util si se realizan mas transformaciones previas), examine el importe la clase ``Pipeline`` de ``sklearn.pipeline``, e intente usarla con ayuda de [los ejemplos de la documentación](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jnc8jwYUCs0k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlCJucFnEX08"
   },
   "source": [
    "* **C)** Examine la instancia de ``SGDRegressor``, y usando el argumento ``loss=\"squared_loss\"`` utilicelo para resolver el inciso **D** anterior de forma iterativa (será util el argumento ``learning_rate=constant`` y ``eta0=`` para comparar con la realizada en el ejercicio anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zjtlsg_EE2i1"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDRegressor?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCO6ax2buzL4YrfpvGc66v",
   "collapsed_sections": [],
   "name": "Ejercicios_05_ModelosLineales",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
